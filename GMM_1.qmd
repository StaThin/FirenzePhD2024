---
title: "Lectures on graphical Markov models"
subtitle: "Part 1"
author: "G. Marchetti"
date: 21 03 2024
format: 
  html:
    theme:  default
    embed-resources: true
    toc: true
    toc-location: left
    self-contained: true
    grid: 
      margin-width: 350px
  pdf:
    documentclass: scrartcl
    fontsize: "10pt"
    number-sections: true
    number-depth: 2
    colorlinks: true
    include-in-header: 
      text: | 
        \usepackage[default]{fontsetup} 
execute: 
  echo: fenced
code-block-border-left: true
callout-appearance: minimal
reference-location: margin
citation-location: margin
bibliography: ref.bib
---
\newcommand{\ci}{\hspace{0.1em}\perp\hspace{-0.95em}\perp\, }
\renewcommand{\b}[1]{\boldsymbol{#1}}

```{=tex}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\setmathfont{NewCMMath-Regular.otf}
```

```{r}
#| echo: false
#| cache: true
```

```{r}
#| echo: false
#| message: false
source("~/Documents/R_packages/cat_regression_chains/RCG//funs_RCG.R")
library("bnlearn") 
library("ggm")
library("igraph")
#library("CauseAndCorrelation")
#library("pwSEM")
library(mnormt)
```

# Variables and conditional independencies

## Introduction

Many investigations involve data on many variables measured on each individual under study. One central notion of statistical thinking is the distinction between the association of two variables, considered on an equal standing, and the directed dependence in which one variable is a response and the other is an explanatory variable. 

## Notation 

Given  a system of random variables
$\b X = (X_1, \dots, X_d)$ we denote the joint probability 
distribution by $p(\b x) = p(x_1, \dots, x_d)$. We use the same notation for the discrete (pmf) or continuous (pdf)  random variables. 

We will consider only two fundamental probability distributions 
for the discrete and continuous cases:

-   **Multinomial**: $\b X \sim$  Multinomial$(1, \boldsymbol{\pi})$
-   **Multivariate normal**: $\boldsymbol{X} \sim N(\boldsymbol{\mu}, \Sigma)$ 


## Discrete data


### Simulation of binary variables in R

Let $X_1, X_2, X_3$ be three binary variables. The joint distribution is
defined by a $2^3$ vector of probabilities $\pi$ that sum to 1. The
single joint probabilities are all $>0$ (no structural zeros).

```{r}
n = 1000
p <- c(0.38, 0.01, 0.07, 0.01, 0.42, 0.02, 0.08, 0.01)
X<- expand.grid(X1 = factor(0:1), 
                X2 = factor(0:1), 
                X3 = factor(0:1))
Z <- rmultinom(n, size = 1, prob = p)
# X <- X[1:length(p) %*% Z,]
cell <- apply(Z, 2, function(x) which(x==1))
data <- X[cell,]
rownames(data) <- 1:n
```

There are 3 types of output: (1) the data matrix, (2) a multidimensional
table and (3) an aggregated data matrix as follows.

The data matrix:

```{r}
head(X) 
tail(X)
```
The multidimensional table
```{r}
table(X)
```
The aggregated data matrix:
```{r}
as.data.frame(table(X))
```


::: {#exm-perinatal}

The following data concern a large sample of mothers ($n$ = 24220) 
on which three binary variables are observed

-   $X_1$ perinatal mortality
-   $X_2$ previous stillbirth
-   $X_3$ skin colour of the mother, considered as a socioeconomic
    surrogate variable.

$X_1$ is the main response, $X_2$ and $X_3$ are potential explanatory
variables.

```{r}
#| echo: false
counts <- c(9148, 270, 1678, 134, 10502, 371, 1963, 154)
X <- expand.grid(X1 = factor(c("no", "yes")), 
                 X2 = factor(c("no", "yes")), 
                 X3 = factor(c("no", "yes")))
colnames(X) <- c("mortality", "previous", "skin-color")
X$Freq <- counts
X
```
We are interested in verifying the dependence of mortality on 
the skin colour of the mother.
:::

## Continuous variables

### Simulation of Gaussian variables in **R**

Generating a sample with $n = 100$ from a 3-dim Gaussian variable with
mean vector zero and a concentration matrix $$
K = \Sigma^{-1}
$$ with a zero concentration $\kappa_{13}$

```{r}
library(mnormt)
K <- matrix(c(1.2, 0.9,   0,
              0.9, 2.7, 1.4,
              0,   1.4, 1.6), 
            3, 3)
dimnames(K) <- list(c("X1","X2", "X3"), 
                    c("X1","X2", "X3"))
Sigma <- solve(K)
dimnames(Sigma) <- list(c("X1","X2", "X3"), 
                        c("X1","X2", "X3"))
round(Sigma, 2)
round(cov2cor(Sigma),2)
X <- rmnorm(n = 100, varcov = Sigma)
```

Above you see the covariance and correlation matrices. In the side @fig-gaussian is shown the scatterplot matrix of the generated data.

```{r}
#| echo: false
#| out-width: "90%"
#| fig-height: 5
#| label: fig-gaussian
#| fig-cap: "Scatterplot matrix of a trivariate normal sample."
#| column: margin
#| message: false
pairs(X)
```

## Conditional independence

Given three random variables (or random vectors) with joint pmf or pdf
$p(x_1, x_2, x_3)$ $X_1$ is conditionally independent of $X_2$ given
$X_3$, written $X_1 \ci X_2 \mid X_3$, if 
$$
p(x_1, x_2|x_3) = p(x_1|x_3)p(x_2|x_3) \quad \text{ for all } x, y
$$ 
for all $z$ such that $p(z)>0$. Of course conditional independence is symmetric:
$$
X_1 \ci X_2 \mid X_3 \iff X_2 \ci X_1 \mid X_3.
$$

### Factorization property

The conditional independence $X_1 \ci  X_2\mid X_3$ is equivalent to the
two factorizations of the joint distribution 
$$
\begin{aligned}
p(x_1, x_2, x_3) &= p(x_1|x_3) p(x_2|x_3)p(x_3)\\
                 &= p(x_1|x_3) p(x_3|x_2)p(x_2)
\end{aligned}
$$
for all $(x_1, x_2, x_3)$.

### Marginal independence
Marginal independence is a quite different relation: two variables
$X_1$ and $X_2$ are marginally independent, denoted by $X_1 \ci X_2 \mid \emptyset$ or simply $X_1 \ci X_2$,  if 
$$
p(x_1, x_2) = p(x_1) p(x_2) 
$$
for all pairs $(x_1, x_2)$. 

>_Notice that marginal independence $X_1 \ci X_2$ does not imply the conditional independence $X_1 \ci X_2 \mid X_3$_. The following is a counterexample.

::: {#exm-radelet}
Consider a $2\times 2\times 2$ table classifying 100 subjects convicted of homicide according to the variables `def_race` (defendant's race),
`vict_race` (victim's race) and `death_pen` (death penalty). 

```{r}
#| echo: false
Freq <- c(37, 8, 9, 38, 3, 2, 1, 2)
X <- expand.grid(def_race=c("white", "black"), 
                 vict_race = c("white", "black") , 
                 death_pen = c("no", "yes"))
df <- cbind(X, Freq)
df
tab <- xtabs(Freq ~ ., df)
```
The marginal table of defendants's race and death penalty  is
```{r}
#| echo: false
margin.table(tab, c(1,3))
```
Thus, there is a marginal independence between defendant's race and death penalty. However, the conditional distribution is the following:
```{r}
#| echo: false
prop.table(ftable(death_pen ~ def_race + vict_race, tab),1)
```
and clearly this shows conditional dependence. 
:::

### Fundamental property of conditional independence

A fundamental property of conditional independence is the following equivalence
$$
X_1 \ci (X_2, X_3) \mid X_4 \iff X_1 \ci X_2 \mid X_4 \quad \text{and} \quad  
X_1 \ci X_3 \mid (X_2, X_4).
$$
This property makes it possible to decompose a CI involving random vectors into a set of pairwise conditional independencies.


This property is said to obey to the so-called *semi-graphoid axioms* ; see @pearl:1988.

In distributions that are *strictly positive* 
an additional property called the **intersection property** holds:
$$
X_1 \ci X_2 \mid (X_3,X_4)  \quad \& \quad X_1 \ci X_3 \mid (X_2, X_4) \implies X_1 \ci (X_2, X_3) \mid X_4.
$$


### Composition property 

There is a further property of CI called the **composition property** that  seems quite natural: 
$$
X_1 \ci X_2 \mid X_4 \quad \& \quad X_1 \ci X_3\mid X_4 \implies X_1 \ci (X_2, X_3) \mid X_4.
$$
This property holds for the family of the multivariate normal distributions. Considering the  simplest case when the variable $X_4$, assume that 
$$
(X_1, X_2, X_3)^T \sim N(\b \mu , \Sigma) \text{ with }
\Sigma = \begin{pmatrix}
\sigma_{11} & 0 & 0 \\
.           & \sigma_{22} & \sigma_{23} \\
.           & .       & \sigma_{33}
\end{pmatrix}.
$$
Then it can be shown that $\sigma_{12} = 0$ and $\sigma_{13} = 0$ are equivalent to $X_1 \ci X_2 \quad \& \quad X_1 \ci X_3$.  But therefore, the joint density can be decomposed as
$p(x_1, x_2, x_3) = p(x_1) p(x_2, x_3)$ implying the joint independence $X_1 \ci (X_2, X_3)$. 

Nevertheless this property does not hold for all distributions. Below, you see a counterexample still for three binary variables.

::: {#exm-composition}
```{r}
#| echo: false 
p <- c(0.20,  0.15,   0.05, 0.10, 0.05, 0.10, 0.20, 0.15)
X <- expand.grid(X1 = c(0, 1), X2 = c(0,1), X3 = c(0,1))
df <- data.frame(X, p)
tab <- xtabs(p ~ X1 + X2 + X3, df)
f <- ftable(X3+X2 ~ X1, tab)
addmargins(as.matrix(f))
```
This $2\times 2\times 2$ table implies two marginal tables with independence (actually they have uniform probability)  

```{r}
#| echo: false
margin.table(tab, c(1,2)) 
margin.table(tab, c(1,3))
```
but it can verified that  $p(x_1, x_2, x_3) \ne p(x_1)p(x_2, x_3)$.
:::

# Graphs representing conditional independencies
In general a graph is a pair of sets $G = (V, E)$ where $V = \{1, \dots, d = |V|\}$ is a set of *nodes/vertices* and $E$ is a set of *edges* 
connecting two nodes $i$ and $j$. The edges can be 

- *arrows*: $i \rightarrow j$
- *full lines*: $i -\!\!\!- j$
- *arcs*: $i \longleftrightarrow j$

>  The arcs are bi-directed edges which I will represent as dashed arcs to better distinguish them from arrows

We consider tipically graphs with at most one edge between two nodes, but there are exceptions. We distiguish several types of graph:

- **directed graphs** containing only arrows
- **undirected graphs** containing only full lines
- **bi-directed graphs** containing only arcs.
- **mixed graphs** containing the above three types of edges. 

Some examples are shown in @fig-graphs. 
```{r}
#| echo: false
#| label: fig-graphs
#| fig-cap: "Left: a directed graph. Center: an undirected graph. Right: a mixed graph."
#| fig-width: 9
#| fig-height: 2
#| fig-column: page-right

par(mfrow = c(1,3))
D <- DAG(X1 ~ X2 + X3, X2 ~ X4, X3 ~ X4, X4 ~ X5)
co <-
structure(c(9, 36, 35, 64, 95, 51, 80, 20, 48, 48), dim = c(5L, 
2L))
drawGraph(D, lwd = 2, coor = co)
G <- UG(~X1*X2 + X2*X3 + X2*X4 + X4*X5 + X4*X6)
co <-
structure(c(9, 32, 9, 69, 95, 92, 26, 49, 77, 50, 20, 80), dim = c(6L, 
2L))
drawGraph(G,  lwd = 2, coor = co)
G[1,2] <- 100; G[2,1] <- 100
G[2,3] <- 100; G[3,2] <- 100
G[2,4] <- 0
G[4,6] <- 10; G[6,4] <- 10
G[4,5] <- 10; G[5,4] <- 10
co <-
structure(c(9, 32, 9, 69, 95, 92, 26, 49, 77, 50, 20, 80), dim = c(6L, 
2L))
drawGraph(G,  lwd = 2, coor = co)
```
### Why graphs?

Probabilistic graphical models are introduced  to represent a set of  conditional independence relationships between the variables. 
The method consists of defining as many nodes as there are variables by indicating with  edges  the dependencies between variables. Missing edges, on the other hand, define the conditional independencies between variables with a conditioning set determined by the structure of the graph. 

To understand how, it is useful to start from a characterization of the variables that are modelled. Typically an investigation is based on $n$  observations collected on  variables that may be distinguished as

- *purely explanatory*, (background) variables
- *intermediate variables*
- *response variables*

:::{#exm-labor}
Consider a  study on labor market where 3 binary variables are observed:
`Hiring`:  the successful job placement of an individual (Yes=1, No=0),  `Qualification`: the university studies 
of the applicant (Classical, Scientific), and the `Gender` of the applicant (F = female, M = male).

It seems natural to predict hiring (`H`) from both qualification (`Q`) 
and gender (`G`). Thus `H` is the *primary response*, and `G` is 
a *purely explanatory variable* (that in principle  cannot be considered a *treatment*). On the other hand it is reasonable to consider `Q` to be 
dependent on gender `G` but a potential influence of hiring `G`. Thus
qualification is called an *intermediate variable*. 

The three variables, by subject-matter considerations, 
follow the order $\texttt{H} < \texttt{Q} < \texttt{G}$ and 
in order to respect the interpretation, the joint distribution 
should be factorized as 
$$
P(\texttt{H} = i, \texttt{Q} = j, \texttt{G} = k) = 
P(\texttt{H} = i \mid  \texttt{Q} = j, \texttt{G} = k)
P(\texttt{Q} = j\mid \texttt{G} = k) P(\texttt{G} = k)
$$
The dependencies in the joint distribution are compactly expressed
bu the directed graph in @fig-firstdag.
```{r}
#| echo: false
#| label: fig-firstdag
#| fig-cap: "The directed graph associated to the distribution of `H` = hiring, `Q` = qualification and `G` = gender."
#| fig-width: 4
#| fig-height: 2
#| column: margin
co <- structure(c(9, 53, 95, 20, 80, 21), dim = 3:2)
drawGraph(DAG(H ~ Q+G, Q ~ G), lwd = 2, coor = co, beta = 5)
```
Each arrow is directed from the explanatory variables to the response or the intermediate variable and represents the dependence specified by the 
factors as follows
$$
\begin{aligned}
\texttt{H} \leftarrow \texttt{Q},  \texttt{H}  \leftarrow \texttt{G}
 &: P(\texttt{H} = i \mid  \texttt{Q} = j, \texttt{G} = k)\\
\texttt{Q}  \leftarrow \texttt{G}  &: P(\texttt{Q} = j\mid \texttt{G} = k)
\end{aligned}
$$
Now suppose that hiring `H` depends only on qualification `Q`. This implies that the first factor can be simplified 
$$
P(\texttt{H} = i \mid  \texttt{Q} = j, \texttt{G} = k) = 
P(\texttt{H} = i \mid  \texttt{Q} = j)
$$
and there is a conditional independence $\texttt{H} \ci \texttt{G} \mid \texttt{Q}$ that can be expressed by deleting 
the edge $\texttt{H} \leftarrow \texttt{G}$  as shown in @fig-missingedge.
```{r}
#| echo: false
#| label: fig-missingedge
#| fig-cap: "The directed graph associated to the distribution of `H` = hiring, `Q` = qualification and `G` = gender, showing the conditional independence of `H` and `G` given `Q`."
#| fig-width: 4
#| fig-height: 2
#| column: margin
co <- structure(c(9, 53, 95, 20, 80, 21), dim = 3:2)
drawGraph(DAG(H ~ Q, Q ~ G), lwd = 2, coor = co, beta = 5)
```
:::


## Directed Acyclic Graphs

The graph in the previous example is a special case of directed acyclic graph (DAG). This kinds of graph are quite useful to deal with asymmetric relationships between variables. The directed acyclic graph models, 
sometimes called *Bayesian networks* are directed graphs 
with *no directed cycles*. 

This means that in such graphs it 
is not possible to start from any node and reach the same node by following the direction of the arrows. For instance, if in the graph of @fig-missingedge we add an edge $\texttt{H} \rightarrow \texttt{G}$ we obtain a directed cycle that is forbidden in DAGs. 
```{r}
#| echo: false
#| label: fig-dag
#| fig-cap: "A directed acyclic graph with $7$ nodes."
#| fig-width: 4
#| fig-height: 3
#| column: margin
g <- DAG(`1` ~ `2`+`3`+`4`, `2` ~ `5`, `3` ~ `4`+`6` , `4` ~ `5`+`6`, `5`~ `7`, `6`~`7`)
rownames(g) <- c("B", "A", "D", "C", "E", "F",  "G")
colnames(g) <- c("B", "A", "D", "C", "E", "F",  "G")
co <-
structure(c(9, 33, 28, 49, 71, 74, 95, 50, 80, 20, 49, 80, 20, 
49), dim = c(7L, 2L))
drawGraph(g, coor = co, alpha = 2, lwd = 2)
```
An example of a  more general DAG is shown in @fig-dag where the nodes are denoted by capital letters and should be interpreted as variables. A DAG is defined essentially by  the set of **parents** of each node, 
that is by the set of nodes that are directly connected to each node. In the graph of @fig-dag we have for example
$$
\begin{array}{cc} 
\text{node}    & \text{pa}(\text{node}) \\ \hline
B & A, D, C \\
A & E\\
D & C, F\\
C & E, F\\
E & G\\
F & G\\
G & \emptyset\\ \hline 
\end{array}
$$
Note that the set of parents can be empty.

The nodes  of a DAG can always be well-ordered, that is we can assign to each node a number so that for each arrow the head number is always smaller than the tail number.
For instance the graph of @fig-dag is well-ordered if the nodes are listed as 
$$
B, A, D, C, E, F, G
$$
Notice that this order is not unique because 
$$
B, D, C, A, F, E, G
$$
is also another possible well-ordering.

If the DAG is well-ordered we can define for each node the set of its *predecessors* containing the nodes that have a higher 
number than that of the node. 
For instance, in the first well-ordering
$$
\text{pre}(D) = \{C,E,F,G\}.
$$

### Factorization
Let $G$ be a DAG with nodes in a specific well-ordering 
associated to the variables $\b X = X_1, \dots, X_d$. If we have a distribution for $\b X$ with density function (in a broader sense)
$p(x_1, \dots, x_d)$ we say that the distribution is Markov to $G$ if 
$$
p(x_1, \dots, x_d) = \prod_{i=1}^d p(x_i | \text{pa}(x_i))  
$$
For example, reconsider the DAG of @fig-dag  where the variables 
are $(B, A, D, C, E, F, G)$. Then a distribution with density 
$p(b,a, d, c, e, f, g)$ (the lowercase symbols indicate the values of the variables) is 
Markov to the DAG if the joint distribution 
has the factorization 
$$
p(b, a, d, c, e, f, g)  = p(b|a,d,c) p(a|e)p(d|c,f)p(e|g) p(f|g) p(g)
$$
Notice that $p(g|\text{pa}(g)) = p(g|\emptyset) \equiv p(g)$. 

>_It can be proved that the distribution is Markov to $G$ if and only if the following **ordered Markov property holds**_:
$$
X_i \ci X_{\text{pre}(i)  \setminus \text{pa}(i)} \mid X_{\text{pa}(u)} 
$$

This theorem implies that the joint distribution that is Markov to 
a DAG must satisfy a set of conditional independencies:  each node 
of the DAG is independent of all the variables that are predecessors but not parents, given its parents.

For the DAG of @fig-dag we have 
$$
\begin{array}{ccc}
\text{node} & \text{pre}(\text{node})  \setminus \text{pa}(\text{node}) &  \text{pa}(\text{node}) \\ \hline
B & E,F,G & A,D,C \\
A & D,C,F,G & E\\
D & E,G & C, F\\
C & G & E,F \\
E & F & G \\ \hline
\end{array}
$$


## Significance tests for CI 

We are interested in testing the CI hypothesis 
$$
X_i \ci X_{b} \mid X_{c} \quad   \text{ for } i = 1 \dots, d
$$
with $c = \text{pa}(i)$ and $b = \text{pre}(i) \setminus \text{pa}(i)$.
We use the likelihood ratio test defined for the 
Gaussian and the Multinomial cases as follows.

- **Gaussian case**
$$
\text{dev}_i = -n \log(\det  S \hat K)
$$
where $S$ is the sample covariance matrix for the variables
$X_i, X_{b}, X_{c}$ and 
$\hat K$ is the inverse of the matrix
$$
\hat S = S_{ic}S_{cc}^{-1} S_{cb}.
$$
The deviance has an asymptotic chi-squared distribution with 
$|i|  |b|$ degrees of freedom.
- **Multinomial case**
$$
\text{dev}_i = \sum_{s\in \mathcal{I}}\sum_{t \in \mathcal{B}}\sum_{u \in \mathcal{C}} \frac{n_{stu}}{n} \log \frac{n_{stu}n_{++u}}{n_{s+u}n_{+tu}}
$$
where $n_{stu}$ are the joint frequencies of the contingency table 
with levels $\mathcal{I} \times \mathcal{B} \times\mathcal{C}$
corresponding to the random variables (or vectors) $X_i$, $X_b$, $X_c$. We make use of the subscript "+" to denote the marginal 
frequencies. 
The deviance has an asymptotic chi-squared distribution with 
$(|\mathcal{I}| - 1)(|\mathcal{B}|-1) |\mathcal{C}|$ degrees of freedom.

::: {#exm-test-perinatal}
To test the independence of mortality and skin color
given the previous still-birth we can use the function `ci.test` in package **bnlearn**. Start from the aggregate table and transform it to a data frame.


```{r}
counts <- c(9148, 270, 1678, 134, 10502, 371, 1963, 154)
X <- expand.grid(X1 = factor(c("no", "yes")), 
                 X2 = factor(c("no", "yes")), 
                 X3 = factor(c("no", "yes")))
colnames(X) <- c("mortality", "previous", "skin_color")
X$Freq <- counts
X
```
Then, the deviance (called `mi`), the degrees of freedom and the p-values are calculated with 
```{r}
ci.test("mortality", "skin_color", "previous", data = tab2data(X)) 
```
:::

$$
\begin{array}{lrrrrrr} 
         & region  &Atlantic   & BC     & Ontario & Prairie & Quebec\\ \hline
children & hincome &           &        &         &       & \\   
absent   & 0       &       2   &     6  &    16   &  2    &  8 \\
         & 1       &       2   &     8  &    17   &  3    & 15\\
present  & 0       &      13   &     9  &    46   &  7    & 20\\
         & 1       &      13   &     6  &    29   & 19    & 22\\ \hline
\end{array}
$$
